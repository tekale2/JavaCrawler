% This is "sig-alternate.tex" V2.0 May 2012
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate-05-2015}
\usepackage{epstopdf}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepgfplotslibrary{external}
\usepackage{blindtext}
\usepackage{listings}
\usepackage{forest}

\begin{document}
%
% --- Author Metadata here ---
%\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Proposed Method of Information Retrieval and Display from the US Federal Register}
%\subtitle{[Extended Abstract]}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Matthew J Wiecek\\
       \affaddr{Texas A\&M University}\\
       \affaddr{College Station, TX}\\
       \email{matthewwiecek@tamu.edu}
% 2nd. author
\alignauthor
Chun-Chan (Bill) Cheng\\
        \affaddr{Texas A\&M University}\\
       \affaddr{College Station, TX}\\
       \email{aznchat@tamu.edu}
% 3rd. author
\alignauthor
Divyesh M Tekale\\
        \affaddr{Texas A\&M University}\\
       \affaddr{College Station, TX}\\
       \email{tekale2@tamu.edu}
}
\setcopyright{rightsretained}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
The US Federal Government posts many Notices, Proposed Rules, Final Rules, Presidential Documents, and other documents of consequence on a daily basis. Being able to search and navigate through all of these documents in a rapid fashion is of the utmost importance in many legal and political fields. We propose a system that will graph the search results in a hierarchical structure to clearly indicate which federal agency published the document. We further propose that the results be color-coded to easily identify the type of document retrieved. 
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003317.10003359.10011699</concept_id>
<concept_desc>Information systems~Presentation of retrieval results</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003371.10003381.10003382</concept_id>
<concept_desc>Information systems~Structured text search</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003371.10010852.10003393</concept_id>
<concept_desc>Information systems~Enterprise search</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Presentation of retrieval results}
\ccsdesc[300]{Information systems~Structured text search}
\ccsdesc[300]{Information systems~Enterprise search}
\printccsdesc

\terms{Theory}

\keywords{Federal Register, Graph Output, Information Retrieval}

\section{Introduction}
The US Federal Register is the official Daily Journal of the United States government. All Federal Notices, Proposed Rules, Final Rules, Presidential Documents, et cetera, are published by the Register. Any document published in the Register carries legal significance as they are considered to constitute legal notice to the public. Being able to rapidly search through the Federal Register and interpret the results is a valuable ability for any legal counsel. 

In particular, the layout of the search results is crucial. Upon presentation, the user should immediately be able to identify the type of document as well as the the publishing agency. Owing to the hierarchical structure of government, a graph structure can quickly represent which agency published a document and where it lies in the federal bureaucracy. A color coding scheme may be used to denote the type of document presented.

\section{Prior Work}
\subsection{Search}
\subsubsection{Crawling}
Crawling the content is the first step to building any search engine. There is no point of having a search engine if there is no content. As the Federal Register has an API available, web crawling becomes simpler. Instead of trying to extract data from HTML, we simply request the data and get a response in easily parsable JSON format. Crawling RESTful services has a long history. Indeed, there exist systems, such as RESTLER that exist to not only crawl a RESTful api, but to also map it out and visualize the various resources the API made available \cite{REST}.
\subsubsection{Indexing}
Indexing is likewise 
\subsection{Visualization}

\section{High Level Overview of System}
\subsection{Subsystems}
The system shall be made up of four distinct subsystems: the crawler, the search engine, and the user interface. The crawler will interact with the Federal Register API to fetch the documents, the indexer will process them into a format that is acceptable to the search engine and the search engine shall serve user queries and return a list of documents that match the query. The user interface shall be responsible for handling user interaction and displaying the search results returned by the search engine.

\subsubsection{Crawler and the Federal Register API}
The Federal Register maintains an API to allow developers access to federal documents.\footnote{The documentation to the API may be found at https://www.federalregister.gov/learn/developers} As the API has a limit of 1000 documents per request, the crawler shall request the full set of each day's documents one day at a time. It is not expected that there shall be more than 1000 documents published by the Federal Government on any given day. The response shall be parsed and returned to the search engine for indexing. 

\subsubsection{Search Engine and Apache Solr}
The search engine used will be Apache Solr. 

\subsubsection{User Interface and the Web Browser}
Users will interact with the search engine via a web interface. The user will be free to input a query via a text box. There will be further filtering options available, such as restricting the search to certain federal departments or agencies. The user query will then be passed to the search engine via an HTML GET request. Upon receipt of the response from the search engine, the web page will be responsible for drawing the graph and color coding the elements. 

\begin{figure}
\centering
\epsfig{file=Solr_Workflow.eps, width = 240pt}
\caption{The sequence of events in the application}
\end{figure}

\subsection{Timeline of Events}
There are two events during the application's life. The initial launch of the application, and a user query.

\subsubsection{Launch of Application}
Upon launch of the application, both Solr and the Web Crawler will launch (see Figure 1). Due to the 1000 item limit per API call imposed by the Federal Register API, the crawler will request documents one day at a time. Once the documents have been fetched and processed, the crawler shall store the documents for future indexing. The crawler will the update its own internal record of which days have already been crawled, and begin crawling the earliest day that has not yet been crawled. This will continue indefinitely until program termination, an error occurs within the crawler, or a user defined amount of days has been crawled.

\subsubsection{User Query}
Once the user enters a query, said query is forwarded via an HTTP GET request to Solr for fulfillment. Solr will return a list of potential documents that match the result with associated meta-data. The web page will render the results and display any spelling recommendations that have been executed. At this point the user will be free to interact with the search results, such as expanding them to get the abstract, or a link to the document itself.

\section{Low Level Overview of Subsystems}
\subsection{Web Crawler \& Federal Register API}
The webcrawler will be written in \emph{Java} and will be composed of several individual subsystems. 
\subsubsection{Query Generation}
The Federal Register maintains an API that is accessible via standard HTTP GET requests. The request has been crafted to request a JSON object containing the links to the XML versions of each published document. As the API sets a maximum limit of 1000 items per request, a query will be generated to request each day's published documents; the assumption is that there will never be more than 1000 unique documents published in any given day. The class \emph{Generate\_Queries} is responsible for generating the web requests. It implements the runnable interface, allowing other threads to run as it generates the queries. The generated query is then pushed into a \emph{BlockingQueue} which is thread-safe so that other threads may execute the queries.

\subsubsection{Fetching \& Parsing JSON Response}
Once the queries have been generated, an HTTP request has to be served over the network to retrieve the JSON object from the Federal Register API that contains the links to the XML versions of that days documents. This is the responsibility of \emph{Get\_List\_Docs}. It fetches the queries from a \emph{BlockingQueue}, sends the GET request, and parses the JSON response. The extracted URLS are put into a different \emph{BlockingQueue} so that other threads may fetch the documents themselves. This class also implements the runnable interface to parallelize the fetching of the JSON responses. As there is no interaction necessary between different \emph{Get\_List\_Docs}, many threads can be spawned to ensure that the network is saturated.

\subsubsection{Retrieval of Published Documents}
Once the URL of the XML version of the document has been retreived, the actual document must be retrieved over the network. The class \emph{Get\_Doc} retrieves the actual document itself. It, also, implements the runnable interface. The number of threads can be increased to ensure that the network is saturated. It fetches the URL to the XML version of the document from a \emph{BlockingQueue}. After fetching the document, this thread will save the fetched document to disk.

\subsection{Indexer}
The indexer will be written in \emph{Java} and will interface with Apache Solr via the \emph{Solrj} library.
\subsubsection{Raw Data \& Crawler Intergration}
It is clear that before data may be indexed, it must first have been crawled. The web crawler dumps each crawled document into its own text file. The indexer will then read the text files from the directory in which the crawler has put the text files; these are stored as strings in a \emph{BlockingQueue}. The indexer threads will parse the XML and extract the relevant fields. The reason for such separation is that it allows indexing and crawling to be done independently. This  minimizes web traffic to the Federal Register and maximizes politeness. If the documents have to be re-indexed due to a change in the schema, we do not need to burden the Federal Register with a request for already crawled documents.

\subsubsection{Indexed Fields \& SolrJ intergration.}
Solr takes in various fields for indexing. There are four well defined types of documents published in the Federal Register: Notices, Proposed Rules, Final Rules, and Presidential Documents. The indexing threads pull the raw strings from the \emph{BlockingQueue} and parse the string as an XML document. First, the type of document is identified. Once the document has been identified, the appropriate XML fields are extracted as defined by the schema in section \ref{Indexing Schema}. The fields are then added to a \emph{SolrInputDocument} and are then passed to the Solr server by the \emph{HTTPSolrServer} class provided by the \emph{Solr} Java API.

\subsubsection{XML Parser}
All the pages we crawled from the federal register was in xml format, so we had to read through the xml data with org.w3c.dom.NodeList library. We parsed the key words \texttt{TYPE}, \texttt{AGENCY}, \texttt{AGENCY TYPE}, \texttt{SUBAGY}, \texttt{SUBJECT}, and for \texttt{PRESDOC} we parsed \texttt{HD}, \texttt{FP} to obtain the information to satisfy the schema in section \ref{Indexing Schema}.\\

\begin{figure} \label{Interface}
\centering
	\includegraphics[width=\linewidth]{"Zoomed in Graph".png}
\caption{Zoomed in UI}
\end{figure}

\subsection{Application}

\subsubsection{User Interface}
The results will be drawn graphically using \emph{JavaScript}. The United States Federal government forms a hierarchical structure with the federal cabinet departments at the highest level, and various executive agencies reporting to them. To represent the publishing agency of a document, we draw out a graph with the Federal Register in the middle and types of documents branching off of it (see schema in Section \ref{Indexing Schema}). The cabinet departments will branch off of the document type and executive agencies branching off from their respective departments, with documents as leaf nodes branching off from their publishing agency.\footnote{Independent executive agencies fall somewhat outside of this hierarchy. At present they will be displayed along cabinet departments.}

As there may sometimes exist joint notices and rules posted by various executive agencies, a single leaf node document may have several parents. To help make the tree readable, different colors will be used to represent different levels of the tree. For instance, Department will be differently coloured than the agencies withing them, see figure \ref{Interface}. Users may hover over any node to get flavour text of the abstract of the document. 

\subsection{Obtaining Search Results}
When a user types in a search query, it is passed to Solr via a simple HTTP get request. Solr returns the search result as a JSON object, which is then drawn as a hierarchical tree. The amount of search results returned is limited to 25 to avoid overwhelming the user by producing an unintelligable graph. The user will have standard next and previous buttons to be able to go through the search results in chunks of 25. 


\section{Statistics on Getting the Documents}
The following figure is the statistics we ran on different number of threads to fetch the data from the webpage.\\


\begin{tikzpicture}[scale=0.95]
\begin{axis}[
    xlabel={Number of Threads},
    ylabel={Time(seconds)},
    xmin=0, xmax=128,
    ymin=0, ymax=360,
    xtick={0,20,40,60,80,100,120},
    ytick={0,60,120,180,240,300,360},
    legend pos=north west,
    ymajorgrids=true,
    grid style=dashed,
]
\legend{340 Files,649 Files, 3978 Files}

\addplot[color=blue, mark=square,]
    coordinates {
    (1,37.110)(2,18.229)(4,9.589)(8,6.331)(16,3.275)(32,3.597)(64,3.032)(128,3.160)
    };
  
\addplot[color=red, mark=*, ]
    coordinates {
    (1,60.337)(2,33.860)(4,17.175)(8,10.096)(16,5.785)(32,4.375)(64,4.359)(128,4.309)
    };
\addplot[color=green, mark=o, ]
    coordinates {
    (1,360.459)(2,186.753)(4,97.714)(8,54.892)(16,29.284)(32,21.127)(64,19.270)(128,17.667)
    }; 
\end{axis}
\end{tikzpicture}

In the figure, we have three lines indicating different numbers of data we crawled with the x-axis as the number of thread and y-axis as the system time. It can be seen that there is a limit in increasing the threads to increase the speed of fetching the webpages. The slope gradually slows down when the thread numbers gets to 32. In conclusion, we fetched 1400 days (11000 documents) in less than three minutes with 100 threads running.  

\section{Indexing Schema} \label{Indexing Schema}
As there are four distinct types of documents published in the Federal Register, it is necessary that a different schema is in place for indexing each type of document. 
\subsection{Notice}
The notice is perhaps the most common document published in the Federal Register. When indexed, documents of this type will contain the following fields:
\begin{description}
\item[Type of Document] shall always be ``Notice''
\item[Cabinet Department] the federal cabinet department which issued the notice
\item[Agency] the agency within the department which issues the notice
\item[Action] description of the type of publication
\item[Summary] a brief summary of the notice
\item[Date] the date of publication in the Federal Register, if available
\end{description}

\subsection{Proposed Rule}
A proposed rule is issued as a guidance document to the public. It describes the current draft of a rule an executive agency is considering imposing. The publication of the draft rule gives the public a chance to provide feedback about the rule as well as giving the public time to prepare for the effects of the potential rule.
\begin{description}
\item[Type of Document] shall always be ``Proposed Rule''
\item[Cabinet Department] the federal cabinet department which issued the proposed rule
\item[Agency] the agency within the department which is issuing the proposed rule
\item[Action] description of the type of publication
\item[Summary] a brief summary of the notice
\item[Date] the date of publication in the Federal Register, if available
\end{description}

\subsection{Rule}
A rule is issued as notice that a proposed rule has been finalized and is now in effect. It serves as both a means to communicate to the public about the new rule, as well as being the legal source of the rule, which may be referenced in court.
\begin{description}
\item[Type of Document] shall always be ``Rule''
\item[Cabinet Department] the federal cabinet department which issued the rule
\item[Agency] the agency within the department which is issuing the rule
\item[Action] description of the type of publication
\item[Summary] a brief summary of the notice
\item[Date] the date of publication in the Federal Register, if available
\end{description}

\subsection{Presidential Document}
A presidential document is one which has been issued by the President of the United States. They may be executive orders, notices, or other noteworthy documents the President has decided to publish. These do not conform to the same structure as other documents published in the Federal Register, and thus, are harder to index.
\begin{description}
\item[Type of Document] shall always be ``Presidential Document''
\item[Header] the header of the document
\item[Title] from which Title of the US Code does this document derive it's authority (if applicable)
\item[Date] the date of publication in the Federal Register, if available
\end{description}

\subsection{Tree Stucture Overview}
\begin{forest}
  for tree={
    font=\ttfamily,
    grow'=0,
    child anchor=west,
    parent anchor=south,
    anchor=west,
    calign=first,
    edge path={
      \noexpand\path [draw, \forestoption{edge}]
      (!u.south west) +(5pt,0) |- node[fill,inner sep=0.0pt] {} (.child anchor)\forestoption{edge label};
    },
    before typesetting nodes={
      if n=1
        {insert before={[,phantom]}}
        {}
    },
    fit=band,
    before computing xy={l=15pt},
  }
[Document
  [Notice
    [Cabinet Department]
    [Agency]
    [Action]
    [Summary]
    [Date]
  ]
   [Proposed Rule
    [Cabinet Department]
    [Agency]
    [Action]
    [Summary]
    [Date]
  ]
 [Rule
    [Cabinet Department]
    [Agency]
    [Action]
    [Summary]
    [Date]
  ]
 [Presidential Document
    [Header]
    [Title]
    [Date]
  ]
]
\end{forest}

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{Report}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%


\end{document}
